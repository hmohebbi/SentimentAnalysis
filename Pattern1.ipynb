{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "Pattern1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oq8PzYHAN8O",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification\n",
        "> # Sentiment Analysis of IMDB Movie Reviews\n",
        "> Pattern Recognition Course Project #1 @<a href=\"http://www.iust.ac.ir/\">IUST</a>\n",
        "\n",
        "<br>\n",
        "Hosein Mohebbi<br>\n",
        "hosein_mohebbi@comp.iust.ac.ir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuWKTGn7hEa1",
        "colab_type": "text"
      },
      "source": [
        "# Project Overview\n",
        "\n",
        ">Text classification is used in many interesting applications such as spam or non-spam email detection, fake news detection, and sentiment analysis. The main aim of this project is to examine different base classifiers in sentiment analysis task as a text classification problem. Sentiment analysis aims to estimate the sentiment polarity of a body of a text solely on its content. To tackle this problem has been tried any possible combination of four approaches to word embedding (BOW, BERT, TF-IDF, Word2Vec) and four base classifiers (Naive Bayes, SVM, Decision Tree, Random Forest) as well as some text pre-processing techniques on the IMDB movie reviews dataset which contains 50K reviews, half of which are positive and the other half negative. This dataset was compiled by <a href=\"http://ai.stanford.edu/~amaas/\">Andrew Maas</a> and can be find here: <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">Large Movie Review Dataset</a>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx557r76Ux5H",
        "colab_type": "text"
      },
      "source": [
        "# Downloading & Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J54WUn8ZU4Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXYoDQj4VXeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -zxvf aclImdb_v1.tar.gz > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXuZg1jAomg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install bert-embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdXLKSbAqgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-xQcgXRig1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install sentence-transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k485f8bacv2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBeYCgcDUQUd",
        "colab_type": "text"
      },
      "source": [
        "# Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FTdlFDxUQUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ipywidgets as widgets\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import itertools\n",
        "import mxnet as mx\n",
        "from bert_embedding import BertEmbedding\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q3l8llsUQUr",
        "colab_type": "text"
      },
      "source": [
        "# Loading DataSet\n",
        "\n",
        "\n",
        "> Dataset has been divided evenly into a training set and a test set. Moreover, each set contains 12.5K positive and 12.5K negative reviews. The training and test data were loaded in Pandas data frames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUXxV-mGUQUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadDataset(data_dir):\n",
        "    \n",
        "    data = {}\n",
        "    for partition in [\"train\", \"test\"]:\n",
        "        data[partition] = []\n",
        "        for sentiment in [\"neg\", \"pos\"]:\n",
        "            lable = 1 if sentiment == \"pos\" else -1\n",
        "\n",
        "            path = os.path.join(data_dir, partition, sentiment)\n",
        "            files = os.listdir(path)\n",
        "            for f_name in files:\n",
        "                with open(os.path.join(path, f_name), \"r\") as f:\n",
        "                    review = f.read()\n",
        "                    data[partition].append([review, lable])\n",
        "\n",
        "    np.random.shuffle(data[\"train\"])\n",
        "    np.random.shuffle(data[\"test\"])\n",
        "    \n",
        "    data[\"train\"] = pd.DataFrame(data[\"train\"],\n",
        "                                 columns=['text', 'sentiment'])\n",
        "    data[\"test\"] = pd.DataFrame(data[\"test\"],\n",
        "                                columns=['text', 'sentiment'])\n",
        "\n",
        "    return data[\"train\"], data[\"test\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4qI30GJUQU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"aclImdb/\"\n",
        "train_data, test_data = loadDataset(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sutlNe12YG2x",
        "colab_type": "text"
      },
      "source": [
        "Here are the first 5 rows of the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Gi1u5koVpCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "9754eece-a676-4c6d-b835-c80ed6f3183c"
      },
      "source": [
        "# Debugging\n",
        "train_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Lynne Ramsey makes arresting images, and Saman...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\"I went to the movies, to see 'Beat Street' / ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Well. this was not a surprise. many people wil...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pyare Mohan can be safely included in the blac...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you're looking to be either offended or amu...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  Lynne Ramsey makes arresting images, and Saman...         -1\n",
              "1  \"I went to the movies, to see 'Beat Street' / ...         -1\n",
              "2  Well. this was not a surprise. many people wil...         -1\n",
              "3  Pyare Mohan can be safely included in the blac...         -1\n",
              "4  If you're looking to be either offended or amu...         -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qF7qiXVUQU6",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning Dataset\n",
        "> Since this dataset scraped from the web, some HTML codes got mixed up with it. So, cleaning up these texts by removing HTML tags is required. Removing numbers, punctuations, and stop words, replacing negative contraction verb with whose complete forms like won't, splitting compound nouns that are made with hyphen like state-of-the-art, and normalizing texts by lowering them would be beneficial.\n",
        "\n",
        "> To remove stop words, the NLTK stop words set have been used. But, some words which have a negative meaning, such as not or nor, have been removed from the set and some contraction patterns like 're or 'm have been added to stop words set.\n",
        "\n",
        "> Due to training of the BERT embedding on Wikipedia data, for this model we allow some of the punctuations which may cause a more reliable embedding like [,/():;] to remain in the text. Moreover, we save !,?, and . to detect the end of the sentence for a later purpose (using BERT for each sentence).\n",
        "\n",
        "> Stemming and lemmatization according to POS tags of words are used for all embedding except BERT.\n",
        "\n",
        "> Finally, we have replaced white spaces with only one space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjozN_Dlr6Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# POS Tagging\n",
        "def NormalizeWithPOS(word_list):\n",
        "  \n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    stemmer = PorterStemmer() \n",
        "    for word, tag in pos_tag(word_list):\n",
        "        if tag.startswith('J'):\n",
        "            w = lemmatizer.lemmatize(word, pos='a')\n",
        "        elif tag.startswith('V'):\n",
        "            w = lemmatizer.lemmatize(word, pos='v')\n",
        "        elif tag.startswith('N'):\n",
        "            w = lemmatizer.lemmatize(word, pos='n')\n",
        "        elif tag.startswith('R'):\n",
        "            w = lemmatizer.lemmatize(word, pos='r')\n",
        "        else:\n",
        "            w = word\n",
        "        w = stemmer.stem(w)\n",
        "        yield w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXdzWU9bUQU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanText(text):\n",
        "    \n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    text = re.sub(r\"[0-9]+\", ' ', text) # TODO: save 0-10 for IMDB rating\n",
        "    text = re.sub(r\"-\", ' ', text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    \n",
        "    text = text.strip().lower()\n",
        "    \n",
        "    if embedding is 'BOW' or embedding is 'TFIDF_NO_STOP' or embedding is 'Word2Vec_NO_STOP':\n",
        "        # Remove Stop words\n",
        "        default_stop_words = set(stopwords.words('english'))\n",
        "        default_stop_words.difference_update({'no', 'not', 'nor', 'too', 'any'})\n",
        "        stop_words = default_stop_words.union({\"'m\", \"n't\", \"'d\", \"'re\", \"'s\",\n",
        "                                               'would','must',\"'ve\",\"'ll\",'may'})\n",
        "    \n",
        "        word_list = word_tokenize(text)\n",
        "        filtered_list = [w for w in word_list if not w in stop_words]\n",
        "        text = ' '.join(filtered_list)\n",
        "    \n",
        "    # Remove other contractions\n",
        "    text = re.sub(r\"'\", ' ', text)\n",
        "    \n",
        "    # Replace punctuations with space\n",
        "    if embedding is 'BERT': # save ! ? . for end of sentences [,/():;]\n",
        "        filters='\"\\'#$%&*+-<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "    else:\n",
        "        filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    translate_dict = dict((i, \" \") for i in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "    \n",
        "    text = ' '.join([w for w in text.split() if len(w)>1])\n",
        "    # Replace multiple space with one space\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    \n",
        "    # Lemmatization & Stemming \n",
        "    if embedding is not 'BERT':\n",
        "        text = ' '.join(NormalizeWithPOS(word_tokenize(text)))\n",
        "  \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDCYc1vjub2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "bba1fc38-019a-44b4-eba6-4b401ccb599d"
      },
      "source": [
        "# Debugging\n",
        "embedding = 'BOW'\n",
        "txt = train_data.iloc[10]['text']\n",
        "print(\"A review example of dataset before cleaning:\")\n",
        "print(txt, end=\"\\n\\n\")\n",
        "print(\"After cleaning:\")\n",
        "print(cleanText(txt))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A review example of dataset before cleaning:\n",
            "Hood of the Living Dead and all of the other movies these guys directed look like they got together and filmed this with their buddies who have zero talent one afternoon when they were bored (lines are completely unrehearsed and unconvincing). I find that 95% of amateur movies and 90% of home video footage is better than this film (although the similarities between them warrant the comparison). \"Hey lets see if anyone is dumb enough to buy our movies!\". Hopefully nobody ELSE wasn't. My apologies to those involved in the flic as this review is somewhat harsh but i was the dope who read your fake reviews and purchased the movie.\n",
            "\n",
            "After cleaning:\n",
            "hood live dead movi guy direct look like get togeth film buddi zero talent one afternoon bore line complet unrehears unconvinc find amateur movi home video footag well film although similar warrant comparison hey let see anyon dumb enough buy movi hope nobodi els not apolog involv flic review somewhat harsh dope read fake review purchas movi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taLGylY9UQVC",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization\n",
        "\n",
        "> To introduce our data to our classifiers except for Decision Tree, we need to convert each review to numeric features: this is vectorization.\n",
        "\n",
        "> <b>Bag Of Words (BOW):</b> In this approach, we make a list of all the unique words in training data called the vocabulary. then, given an input text, it outputs a numerical vector that counts each word of the vocabulary.</br> For example:</br>\n",
        "Training data:</br> ['It was the best of times', 'It was the worst of times']\n",
        "</br>=></br> Vocabulary: ['It', 'was', 'the', 'best', 'of', 'times', 'worst']\n",
        "</br>Test data:</br> 'It was the age of the wisdom' ->\n",
        "[1,1,2,0,1,0,0]</br>\n",
        "As we can see, the values for 'best', 'times', and 'worst' are 0 because these words did not appear in the test data.\n",
        "\n",
        "> To use BOW vectorization in Python, we can use the CountVectorizer function from the scikit-learn library. We pass our custom clean text function to remove useless words of training data thereby reducing the size of the BOW vectors until 49812.\n",
        "\n",
        "> <b>Bidirectional Encoder Representations  from Transformers (<a href=\"https://arxiv.org/abs/1810.04805\">BERT</a>):</b>\n",
        "BERT, published by Google, is pre-trained language model word representation as a vector with the size of 768.</br>\n",
        "\n",
        "> In this project, we used BERT embedding twice and report them independently. once, we tokenized each training data into words, then calculate BERT embedding of each word, and finally, mean all the BERT vectors of the words as a review representation. In a second way, we tokenized each training data into sentences and computed the mean of BERT embedding of the sentences for each training data.\n",
        "\n",
        "> To utilize BERT embedding  for two mentioned purposes, we have used these two libraries respectively: </br>\n",
        "<a href= \"https://pypi.org/project/bert-embedding/\">bert-embedding 1.0.1</a> </br>\n",
        "<a href= \"https://github.com/UKPLab/sentence-transformers\">Sentence Transformers</a>\n",
        "\n",
        "> <b>TFIDF:</b>\n",
        "\n",
        "> <b>Word2Vec</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGj2xX0GjB_4",
        "colab_type": "text"
      },
      "source": [
        "#BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JHLEqM7UQVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = 'BOW'\n",
        "# , max_features=30000\n",
        "vectorizer = CountVectorizer(preprocessor=cleanText)\n",
        "\n",
        "bow_training_features = vectorizer.fit_transform(train_data[\"text\"])    \n",
        "bow_test_features = vectorizer.transform(test_data[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6FCe5RWyYvL",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF\n",
        "\n",
        "> With Stop Words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZ7ehXcIyYOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = 'TFIDF_WITH_STOP'\n",
        "vectorizer = TfidfVectorizer(preprocessor=cleanText)\n",
        "\n",
        "tfidf_training_features = vectorizer.fit_transform(train_data[\"text\"])    \n",
        "tfidf_test_features = vectorizer.transform(test_data[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQmGbk67t0C8",
        "colab_type": "text"
      },
      "source": [
        "# TF-IDF\n",
        "\n",
        "> Without Stop Words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AV8MYcdutzM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = 'TFIDF_NO_STOP'\n",
        "vectorizer = TfidfVectorizer(preprocessor=cleanText)\n",
        "\n",
        "tfidf_NO_STOP_training_features = vectorizer.fit_transform(train_data[\"text\"])    \n",
        "tfidf_NO_STOP_test_features = vectorizer.transform(test_data[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVuZfCgZu8NJ",
        "colab_type": "text"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X64z6gFQh94L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning before BERT\n",
        "embedding = 'BERT'\n",
        "\n",
        "train_data['clean_text'] = train_data['text'].apply(cleanText)\n",
        "test_data['clean_text'] = test_data['text'].apply(cleanText)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvtD8-HdjDyg",
        "colab_type": "text"
      },
      "source": [
        "(BERT) Word Tokenization Version\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PHBkdKZvdCg",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBNKsUVYurap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean(z):\n",
        "    return sum(itertools.chain(z))/len(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl5lU9f_WfRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embeddToBERT(text):\n",
        "    sentences = re.split('!|\\?|\\.',text)\n",
        "    sentences = list(filter(None, sentences)) \n",
        "\n",
        "    if bert_version == 'WORD':\n",
        "        result = bert(sentences, 'avg') # avg is determined to handle OOV\n",
        "    \n",
        "        bert_vocabs_of_sentence = []\n",
        "        for sentence in range(len(result)):\n",
        "            for word in range(len(result[sentence][1])):\n",
        "                bert_vocabs_of_sentence.append(result[sentence][1][word])\n",
        "        feature = [mean(x) for x in zip(*bert_vocabs_of_sentence)]\n",
        "\n",
        "    elif bert_version == 'SENTENCE':\n",
        "        result = bert_transformers.encode(sentences)\n",
        "        feature = [mean(x) for x in zip(*result)]\n",
        "  \n",
        "    return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv6j4UmHj4yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = mx.gpu(0)\n",
        "bert = BertEmbedding(ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zgeeBX0231r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_version = 'WORD'\n",
        "bert_word_training_features = train_data['clean_text'].apply(embeddToBERT)\n",
        "bert_word_test_features = test_data['clean_text'].apply(embeddToBERT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwBMnzSWmNP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = [x for x in bert_word_training_features.transpose()]\n",
        "bert_word_training_features = np.asarray(feature)\n",
        "\n",
        "feature = [x for x in bert_word_test_features.transpose()]\n",
        "bert_word_test_features = np.asarray(feature)\n",
        "\n",
        "print(bert_word_training_features.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ennfBNGwjUqx",
        "colab_type": "text"
      },
      "source": [
        "(BERT) Sentence Tokenization Version\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggl6x5kmmkHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_transformers = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY37DG10kAm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_version = 'SENTENCE'\n",
        "bert_sentence_training_features = train_data['clean_text'].apply(embeddToBERT)\n",
        "bert_sentence_test_features = test_data['clean_text'].apply(embeddToBERT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV3-Vy9akppq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = [x for x in bert_sentence_training_features.transpose()]\n",
        "bert_sentence_training_features = np.asarray(feature)\n",
        "\n",
        "feature = [x for x in bert_sentence_test_features.transpose()]\n",
        "bert_sentence_test_features = np.asarray(feature)\n",
        "\n",
        "print(bert_sentence_training_features.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRVl3eAoWnWX",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvVoVtNQXTME",
        "colab_type": "text"
      },
      "source": [
        "# Model & Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL41dcagW1ZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM \n",
        "model = SVC(kernel ='linear', C = 1)\n",
        "\n",
        "# Training \n",
        "model.fit(bow_training_features, train_data[\"sentiment\"])\n",
        "\n",
        "# Evaluation\n",
        "y_pred = model.predict(bow_test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zISF4GZUXuIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive Bayes classifier\n",
        "# use toarray() for BOW embedding\n",
        "model = GaussianNB()\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ7WCrVEX59x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision tree classifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhQkqcY_YpWb",
        "colab_type": "text"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nB0lRUTUQVl",
        "colab_type": "code",
        "outputId": "d0f0e79a-7248-4d38-dbae-8e84950f530e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
        "# Result\n",
        "print(\"Accuracy: {:.2f}\".format(acc*100))\n",
        "cm = confusion_matrix(test_data[\"sentiment\"],y_pred)\n",
        "print(cm)\n",
        "print(classification_report(test_data[\"sentiment\"],y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 75.64\n",
            "[[10032  2468]\n",
            " [ 3623  8877]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.73      0.80      0.77     12500\n",
            "           1       0.78      0.71      0.74     12500\n",
            "\n",
            "    accuracy                           0.76     25000\n",
            "   macro avg       0.76      0.76      0.76     25000\n",
            "weighted avg       0.76      0.76      0.76     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jN7xUAOREvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}