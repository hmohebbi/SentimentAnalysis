{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "Pattern1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oq8PzYHAN8O",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification\n",
        "> # Sentiment Analysis of IMDB Movie Reviews\n",
        "\n",
        "\n",
        "\n",
        "Pattern Recognition Course Project #1 @IUST\n",
        "<br><br>\n",
        "Hosein Mohebbi<br>\n",
        "hosein_mohebbi@comp.iust.ac.ir\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuWKTGn7hEa1",
        "colab_type": "text"
      },
      "source": [
        "# Project Overview\n",
        "\n",
        "Text classification is used in many interesting applications such as spam or non-spam email detection, fake news detection, and sentiment analysis. The main aim of this project is to examine different base classifiers in sentiment analysis task as a text classification problem. Sentiment analysis aims to estimate the sentiment polarity of a body of a text solely on its content. To tackle this problem has been tried any possible combination of four approaches to word embedding (BOW, BERT, TF-IDF, Word2Vec) and four base classifiers (Naive Bayes, SVM, Decision Tree, Random Forest) as well as some text pre-processing techniques on the IMDB movie reviews dataset which contains 50K reviews, half of which are positive and the other half negative. This dataset was compiled by <a href=\"http://ai.stanford.edu/~amaas/\">Andrew Maas</a> and can be find here: <a href=\"http://ai.stanford.edu/~amaas/data/sentiment/\">Large Movie Review Dataset</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx557r76Ux5H",
        "colab_type": "text"
      },
      "source": [
        "# Downloading & Installing Prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J54WUn8ZU4Lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXYoDQj4VXeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -zxvf aclImdb_v1.tar.gz > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhXuZg1jAomg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install bert-embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfdXLKSbAqgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install mxnet-cu100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k485f8bacv2d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBeYCgcDUQUd",
        "colab_type": "text"
      },
      "source": [
        "# Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FTdlFDxUQUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ipywidgets as widgets\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import itertools\n",
        "import mxnet as mx\n",
        "from bert_embedding import BertEmbedding\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q3l8llsUQUr",
        "colab_type": "text"
      },
      "source": [
        "# Loading DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUXxV-mGUQUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadDataset(data_dir):\n",
        "    \n",
        "    data = {}\n",
        "    for partition in [\"train\", \"test\"]:\n",
        "        data[partition] = []\n",
        "        for sentiment in [\"neg\", \"pos\"]:\n",
        "            lable = 1 if sentiment == \"pos\" else -1\n",
        "\n",
        "            path = os.path.join(data_dir, partition, sentiment)\n",
        "            files = os.listdir(path)\n",
        "            for f_name in files:\n",
        "                with open(os.path.join(path, f_name), \"r\") as f:\n",
        "                    review = f.read()\n",
        "                    data[partition].append([review, lable])\n",
        "\n",
        "    np.random.shuffle(data[\"train\"])\n",
        "    np.random.shuffle(data[\"test\"])\n",
        "    \n",
        "    data[\"train\"] = pd.DataFrame(data[\"train\"],\n",
        "                                 columns=['text', 'sentiment'])\n",
        "    data[\"test\"] = pd.DataFrame(data[\"test\"],\n",
        "                                columns=['text', 'sentiment'])\n",
        "\n",
        "    return data[\"train\"], data[\"test\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4qI30GJUQU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"aclImdb/\"\n",
        "train_data, test_data = loadDataset(data_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qF7qiXVUQU6",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning Dataset\n",
        "Removing HTML tags and punctuation as well as Lowering text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjozN_Dlr6Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# POS Tagging\n",
        "def NormalizeWithPOS(word_list):\n",
        "  \n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    stemmer = PorterStemmer() \n",
        "    for word, tag in pos_tag(word_list):\n",
        "        if tag.startswith('J'):\n",
        "            w = lemmatizer.lemmatize(word, pos='a')\n",
        "        elif tag.startswith('V'):\n",
        "            w = lemmatizer.lemmatize(word, pos='v')\n",
        "        elif tag.startswith('N'):\n",
        "            w = lemmatizer.lemmatize(word, pos='n')\n",
        "        elif tag.startswith('R'):\n",
        "            w = lemmatizer.lemmatize(word, pos='r')\n",
        "        else:\n",
        "            w = word\n",
        "        w = stemmer.stem(w)\n",
        "        yield w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXdzWU9bUQU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanText(text):\n",
        "    \n",
        "    text = re.sub(r'<.*?>', ' ', text)\n",
        "    text = re.sub(r\"[0-9]+\", ' ', text) # TODO: save 0-10 for IMDB rating\n",
        "    text = re.sub(r\"-\", ' ', text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"can not\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    \n",
        "    text = text.strip().lower()\n",
        "    \n",
        "    if embedding is 'BOW':\n",
        "        # Remove Stop words\n",
        "        default_stop_words = set(stopwords.words('english'))\n",
        "        default_stop_words.difference_update({'no', 'not', 'nor', 'too', 'any'})\n",
        "        stop_words = default_stop_words.union({\"'m\", \"n't\", \"'d\", \"'re\", \"'s\",\n",
        "                                               'would','must',\"'ve\",\"'ll\",'may'})\n",
        "    \n",
        "        word_list = word_tokenize(text)\n",
        "        filtered_list = [w for w in word_list if not w in stop_words]\n",
        "        text = ' '.join(filtered_list)\n",
        "    \n",
        "    # Remove other contractions\n",
        "    text = re.sub(r\"'\", ' ', text)\n",
        "    \n",
        "    # Replace punctuations with space\n",
        "    if embedding is 'BERT': # save ! ? . for end of sentences [,/():;]\n",
        "        filters='\"\\'#$%&*+-<=>@[\\\\]^_`{|}~\\t\\n'\n",
        "    else:\n",
        "        filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "    translate_dict = dict((i, \" \") for i in filters)\n",
        "    translate_map = str.maketrans(translate_dict)\n",
        "    text = text.translate(translate_map)\n",
        "    \n",
        "    text = ' '.join([w for w in text.split() if len(w)>1])\n",
        "    # Replace multiple space with one space\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    \n",
        "    # Lemmatization & Stemming \n",
        "    if embedding is 'BOW':\n",
        "        text = ' '.join(NormalizeWithPOS(word_tokenize(text)))\n",
        "  \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDCYc1vjub2T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Debugging\n",
        "txt = train_data.iloc[10]['text']\n",
        "a = cleanText(txt)\n",
        "print(txt, end=\"\\n\\n\")\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taLGylY9UQVC",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBNKsUVYurap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean(z):\n",
        "    return sum(itertools.chain(z))/len(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl5lU9f_WfRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def embeddToBERT(text):\n",
        "    sentences = re.split('!|\\?|\\.',text)\n",
        "    sentences = list(filter(None, sentences)) \n",
        "    result = bert(sentences, 'avg')\n",
        "    \n",
        "    bert_vocabs_of_sentence = []\n",
        "    for sentence in range(len(result)):\n",
        "        for word in range(len(result[sentence][1])):\n",
        "            bert_vocabs_of_sentence.append(result[sentence][1][word])\n",
        "\n",
        "    feature = [mean(x) for x in zip(*bert_vocabs_of_sentence)]\n",
        "    return feature"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv6j4UmHj4yH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ctx = mx.gpu(0)\n",
        "bert = BertEmbedding(ctx=ctx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X64z6gFQh94L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cleaning before BERT\n",
        "embedding = 'BERT'\n",
        "\n",
        "train_data['clean_text'] = train_data['text'].apply(cleanText)\n",
        "test_data['clean_text'] = test_data['text'].apply(cleanText)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zgeeBX0231r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT Embedding\n",
        "training_features = train_data['clean_text'].apply(embeddToBERT)\n",
        "test_features = test_data['clean_text'].apply(embeddToBERT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwBMnzSWmNP_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature = [x for x in training_features.transpose()]\n",
        "training_features = np.asarray(feature)\n",
        "\n",
        "feature = [x for x in test_features.transpose()]\n",
        "test_features = np.asarray(feature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXYQcGO1BHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Debugging\n",
        "text = train_data.iloc[10]['clean_text']\n",
        "\n",
        "sentences = re.split('!|\\?|\\.',text)\n",
        "sentences = list(filter(None, sentences)) \n",
        "result = bert(sentences, 'avg')\n",
        "    \n",
        "bert_vocabs_of_sentence = []\n",
        "for sentence in range(len(result)):\n",
        "    for word in range(len(result[sentence][1])):\n",
        "        bert_vocabs_of_sentence.append(result[sentence][1][word])\n",
        "\n",
        "feature = [mean(x) for x in zip(*bert_vocabs_of_sentence)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o_Jr4DyOBh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt = [\"these were sent by me\",\"they were happy\", \"he left\"]\n",
        "r = bert(txt, 'avg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu-mbDKbO1yy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r1 = r[0][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lmVFagdP1Nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r2 = r[1][1][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HDtyYaGP1qA",
        "colab_type": "code",
        "outputId": "d4766ecb-3676-4cbb-d974-61270c22a852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(r1[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs8NivFb-c02",
        "colab_type": "code",
        "outputId": "78425656-8076-4665-b0c7-6266abc47ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "l1 = [1.2,2,3,4]\n",
        "l2 = [10,20,30,40]\n",
        "l3 = [-2,-1,0,1]\n",
        "\n",
        "l = []\n",
        "l.append(l1)\n",
        "l.append(l2)\n",
        "l.append(l3)\n",
        "\n",
        "feature = [mean(x) for x in zip(*l)]\n",
        "feature"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.0666666666666664, 7.0, 11.0, 15.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FKaiVE4Ms-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#result[sentence][1][word]\n",
        "result[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLnNkC46XNCl",
        "colab_type": "code",
        "outputId": "fcc63184-7730-45dc-b430-2eb98a094439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.asarray(feature).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcGmNiUYgQQf",
        "colab_type": "code",
        "outputId": "1686d888-9220-494a-b591-190903319800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(result[sentence][1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2lxYtWNvbwO",
        "colab_type": "code",
        "outputId": "5aee3e1c-b269-41aa-fc12-d62379be1cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_features.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogt4d-9m1kUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JHLEqM7UQVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bag Of Words (BOW)\n",
        "embedding = 'BOW'\n",
        "# , max_features=30000\n",
        "vectorizer = CountVectorizer(preprocessor=cleanText)\n",
        "\n",
        "training_features_bow = vectorizer.fit_transform(train_data[\"text\"])    \n",
        "test_features_bow = vectorizer.transform(test_data[\"text\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmxWcw2drO-o",
        "colab_type": "code",
        "outputId": "4cb9e2f1-b539-45c0-a163-65ea0638bcd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_features.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNSAh-RdVy23",
        "colab_type": "code",
        "outputId": "a41edf59-3ddb-4e73-b9e0-28473b53b71e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(training_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OspmKZvUQVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BERT Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRVl3eAoWnWX",
        "colab_type": "text"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvVoVtNQXTME",
        "colab_type": "text"
      },
      "source": [
        "# Model & Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL41dcagW1ZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SVM \n",
        "param_grid = [{'kernel': ['rbf'], 'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "                     'C': [1, 10, 100, 1000]},\n",
        "                    {'kernel': ['linear'], 'C': [0.1, 1, 10, 100]}]\n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, cv=5)\n",
        "\n",
        "# Training \n",
        "grid.fit(training_features, train_data[\"sentiment\"])\n",
        "\n",
        "# Evaluation\n",
        "y_pred = grid.predict(test_features)\n",
        "\n",
        "print(grid.best_params_) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zISF4GZUXuIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Naive Bayes classifier\n",
        "# use toarray() for BOW embedding\n",
        "model = GaussianNB()\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ7WCrVEX59x",
        "colab_type": "code",
        "outputId": "156dfe6c-fd47-447d-a985-9f5808f42f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "source": [
        "# Decision tree classifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(training_features, train_data[\"sentiment\"])\n",
        "y_pred = model.predict(test_features)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-e2e56d0aa80e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mFortran\u001b[0m \u001b[0mcontiguous\u001b[0m \u001b[0mbut\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;32mnot\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m \u001b[0mcontiguous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m     \u001b[0mExamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhQkqcY_YpWb",
        "colab_type": "text"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nB0lRUTUQVl",
        "colab_type": "code",
        "outputId": "d0f0e79a-7248-4d38-dbae-8e84950f530e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "acc = accuracy_score(test_data[\"sentiment\"], y_pred)\n",
        "# Result\n",
        "print(\"Accuracy: {:.2f}\".format(acc*100))\n",
        "cm = confusion_matrix(test_data[\"sentiment\"],y_pred)\n",
        "print(cm)\n",
        "print(classification_report(test_data[\"sentiment\"],y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 75.64\n",
            "[[10032  2468]\n",
            " [ 3623  8877]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.73      0.80      0.77     12500\n",
            "           1       0.78      0.71      0.74     12500\n",
            "\n",
            "    accuracy                           0.76     25000\n",
            "   macro avg       0.76      0.76      0.76     25000\n",
            "weighted avg       0.76      0.76      0.76     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jN7xUAOREvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}